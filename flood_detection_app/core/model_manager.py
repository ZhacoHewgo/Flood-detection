"""
æ¨¡å‹ç®¡ç†å™¨
Model Manager for PyTorch Models
"""

import os
import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import cv2
from ultralytics import YOLO
import torchvision.transforms as transforms
import threading
import gc
from functools import lru_cache
import time

from .data_models import Detection, BoundingBox, ModelConfig
from .exceptions import ModelLoadError, InferenceError, ConfigurationError
from .config import config_manager


class ModelManager:
    """æ·±åº¦å­¦ä¹ æ¨¡å‹ç®¡ç†å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        self.vehicle_models: Dict[str, Any] = {}
        self.water_models: Dict[str, Any] = {}
        self.current_vehicle_model: Optional[str] = None
        self.current_water_model: Optional[str] = None
        self.device = self._get_device()
        
        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        self._model_cache = {}
        self._inference_lock = threading.Lock()
        self._warmup_done = False
        self._batch_size = 1
        
        # ä¼˜åŒ–çš„é¢„å¤„ç†å˜æ¢ - ä½¿ç”¨æ›´å¿«çš„æ’å€¼æ–¹æ³•
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((640, 640), interpolation=transforms.InterpolationMode.BILINEAR),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # è®¾ç½®PyTorchæ€§èƒ½ä¼˜åŒ–
        self._setup_torch_optimizations()
    
    def _get_device(self) -> torch.device:
        """è·å–å¯ç”¨çš„è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            # è®¾ç½®GPUå†…å­˜ä¼˜åŒ–
            torch.cuda.empty_cache()
            if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
                torch.cuda.set_per_process_memory_fraction(0.8)  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨
        else:
            device = torch.device("cpu")
            print("ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
            # CPUä¼˜åŒ–è®¾ç½®
            torch.set_num_threads(min(4, torch.get_num_threads()))  # é™åˆ¶CPUçº¿ç¨‹æ•°
        return device
    
    def _setup_torch_optimizations(self):
        """è®¾ç½®PyTorchæ€§èƒ½ä¼˜åŒ–"""
        # å¯ç”¨cudnnåŸºå‡†æµ‹è¯•ä»¥ä¼˜åŒ–å·ç§¯æ“ä½œ
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    
    def load_models(self) -> bool:
        """åŠ è½½æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        success = True
        
        # åŠ è½½è½¦è¾†æ£€æµ‹æ¨¡å‹
        for model_config in config_manager.config.vehicle_models:
            try:
                print(f"â³ æ­£åœ¨åŠ è½½è½¦è¾†æ£€æµ‹æ¨¡å‹: {model_config.name}...")
                self._load_vehicle_model(model_config)
                print(f"âœ… è½¦è¾†æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ: {model_config.name}")
            except Exception as e:
                print(f"âŒ è½¦è¾†æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {model_config.name} - {e}")
                success = False
        
        # åŠ è½½æ°´é¢åˆ†å‰²æ¨¡å‹
        for model_config in config_manager.config.water_models:
            try:
                print(f"â³ æ­£åœ¨åŠ è½½æ°´é¢åˆ†å‰²æ¨¡å‹: {model_config.name}...")
                self._load_water_model(model_config)
                print(f"âœ… æ°´é¢åˆ†å‰²æ¨¡å‹åŠ è½½æˆåŠŸ: {model_config.name}")
            except Exception as e:
                print(f"âŒ æ°´é¢åˆ†å‰²æ¨¡å‹åŠ è½½å¤±è´¥: {model_config.name} - {e}")
                success = False
        
        # è®¾ç½®é»˜è®¤æ¨¡å‹
        if self.vehicle_models:
            self.current_vehicle_model = list(self.vehicle_models.keys())[0]
        if self.water_models:
            self.current_water_model = list(self.water_models.keys())[0]
        
        # æ‰§è¡Œæ¨¡å‹é¢„çƒ­
        if success and self.current_vehicle_model and self.current_water_model:
            self._warmup_models()
        
        return success
    
    def _warmup_models(self):
        """é¢„çƒ­æ¨¡å‹ä»¥æé«˜é¦–æ¬¡æ¨ç†é€Ÿåº¦"""
        if self._warmup_done:
            return
            
        try:
            print("æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
            start_time = time.time()
            
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥è¿›è¡Œé¢„çƒ­
            dummy_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            
            # é¢„çƒ­è½¦è¾†æ£€æµ‹æ¨¡å‹
            if self.current_vehicle_model:
                try:
                    self.predict_vehicles(dummy_image)
                except:
                    pass  # å¿½ç•¥é¢„çƒ­æ—¶çš„é”™è¯¯
            
            # é¢„çƒ­æ°´é¢åˆ†å‰²æ¨¡å‹
            if self.current_water_model:
                try:
                    self.predict_water(dummy_image)
                except:
                    pass  # å¿½ç•¥é¢„çƒ­æ—¶çš„é”™è¯¯
            
            self._warmup_done = True
            warmup_time = time.time() - start_time
            print(f"æ¨¡å‹é¢„çƒ­å®Œæˆï¼Œè€—æ—¶: {warmup_time:.2f}s")
            
        except Exception as e:
            print(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
    
    @lru_cache(maxsize=32)
    def _get_cached_preprocessing(self, image_shape: Tuple[int, int], model_type: str) -> Any:
        """ç¼“å­˜é¢„å¤„ç†å‚æ•°"""
        # è¿™é‡Œå¯ä»¥ç¼“å­˜ä¸€äº›é¢„å¤„ç†ç›¸å…³çš„è®¡ç®—ç»“æœ
        return {
            'input_shape': image_shape,
            'model_type': model_type,
            'cached_at': time.time()
        }
    
    def _load_vehicle_model(self, model_config: ModelConfig):
        """åŠ è½½è½¦è¾†æ£€æµ‹æ¨¡å‹ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        if not os.path.exists(model_config.file_path):
            raise ModelLoadError(model_config.file_path, "æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        try:
            if "yolov11" in model_config.name.lower():
                # ä½¿ç”¨UltralyticsåŠ è½½YOLOv11æ¨¡å‹
                try:
                    model = YOLO(model_config.file_path)
                except Exception as e:
                    if "weights_only" in str(e):
                        # ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å…è®¸ä¸å®‰å…¨çš„åŠ è½½
                        old_weights_only = os.environ.get('TORCH_WEIGHTS_ONLY', None)
                        os.environ['TORCH_WEIGHTS_ONLY'] = 'False'
                        try:
                            model = YOLO(model_config.file_path)
                        finally:
                            if old_weights_only is not None:
                                os.environ['TORCH_WEIGHTS_ONLY'] = old_weights_only
                            else:
                                os.environ.pop('TORCH_WEIGHTS_ONLY', None)
                    else:
                        raise e
                
                model.to(self.device)
                
                # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
                if hasattr(model.model, 'half') and self.device.type == 'cuda':
                    model.model.half()  # ä½¿ç”¨åŠç²¾åº¦æ¨ç†
                    
            elif "rt-detr" in model_config.name.lower() or "rtdetr" in model_config.name.lower():
                # RT-DETRæ¨¡å‹ä½¿ç”¨YOLOæ¥å£åŠ è½½
                try:
                    print(f"æ­£åœ¨ä½¿ç”¨YOLOæ¥å£åŠ è½½RT-DETR: {model_config.file_path}")
                    model = YOLO(model_config.file_path)
                    print(f"YOLOåŠ è½½å®Œæˆï¼Œæ¨¡å‹ç±»å‹: {type(model)}")
                    model.to(self.device)
                    
                    # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
                    if hasattr(model.model, 'half') and self.device.type == 'cuda':
                        model.model.half()  # ä½¿ç”¨åŠç²¾åº¦æ¨ç†
                    
                    print(f"âœ… RT-DETRæ¨¡å‹é€šè¿‡YOLOæ¥å£åŠ è½½æˆåŠŸï¼Œæœ€ç»ˆç±»å‹: {type(model)}")
                    
                except Exception as e:
                    raise ModelLoadError(model_config.file_path, f"RT-DETRåŠ è½½å¤±è´¥: {e}")
                
            else:
                # é€šç”¨PyTorchæ¨¡å‹åŠ è½½ï¼Œå¤„ç†weights_onlyé—®é¢˜
                try:
                    model = torch.load(model_config.file_path, map_location=self.device, weights_only=False)
                except TypeError:
                    # æ—§ç‰ˆæœ¬PyTorchä¸æ”¯æŒweights_onlyå‚æ•°
                    model = torch.load(model_config.file_path, map_location=self.device)
                
                if hasattr(model, 'eval'):
                    model.eval()
                
                # å¯ç”¨åŠç²¾åº¦æ¨ç†ï¼ˆå¦‚æœæ”¯æŒï¼‰
                if self.device.type == 'cuda' and hasattr(model, 'half'):
                    model.half()
            
            # ç¼–è¯‘æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦ï¼ˆPyTorch 2.0+ï¼‰
            # åªå¯¹å¯è°ƒç”¨çš„PyTorchæ¨¡å‹è¿›è¡Œç¼–è¯‘ï¼Œè·³è¿‡YOLOå’ŒRT-DETRæ¨¡å‹
            if (hasattr(torch, 'compile') and 
                not isinstance(model, YOLO) and 
                "ultralytics" not in str(type(model)).lower() and
                hasattr(model, '__call__') and 
                hasattr(model, 'parameters') and
                callable(model) and
                "rt-detr" not in model_config.name.lower() and
                "rtdetr" not in model_config.name.lower()):
                try:
                    model = torch.compile(model, mode='reduce-overhead')
                    print(f"âœ… è½¦è¾†æ£€æµ‹æ¨¡å‹ç¼–è¯‘æˆåŠŸ: {model_config.name}")
                except Exception as e:
                    print(f"âš ï¸ è½¦è¾†æ£€æµ‹æ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {e}")
            
            print(f"å­˜å‚¨æ¨¡å‹ {model_config.name}ï¼Œç±»å‹: {type(model)}")
            self.vehicle_models[model_config.name] = model
            
        except Exception as e:
            raise ModelLoadError(model_config.file_path, str(e))
    
    def _load_water_model(self, model_config: ModelConfig):
        """åŠ è½½æ°´é¢åˆ†å‰²æ¨¡å‹ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        if not os.path.exists(model_config.file_path):
            raise ModelLoadError(model_config.file_path, "æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        try:
            if "yolov11" in model_config.name.lower():
                # YOLOv11åˆ†å‰²æ¨¡å‹ï¼Œå¤„ç†PyTorch 2.6å…¼å®¹æ€§
                try:
                    model = YOLO(model_config.file_path)
                except Exception as e:
                    if "weights_only" in str(e):
                        # ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å…è®¸ä¸å®‰å…¨çš„åŠ è½½
                        old_weights_only = os.environ.get('TORCH_WEIGHTS_ONLY', None)
                        os.environ['TORCH_WEIGHTS_ONLY'] = 'False'
                        try:
                            model = YOLO(model_config.file_path)
                        finally:
                            if old_weights_only is not None:
                                os.environ['TORCH_WEIGHTS_ONLY'] = old_weights_only
                            else:
                                os.environ.pop('TORCH_WEIGHTS_ONLY', None)
                    else:
                        raise e
                
                model.to(self.device)
                
                # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
                if hasattr(model.model, 'half') and self.device.type == 'cuda':
                    model.model.half()  # ä½¿ç”¨åŠç²¾åº¦æ¨ç†
                    
            elif "deeplabv3" in model_config.name.lower():
                # DeepLabV3åˆ†å‰²æ¨¡å‹ï¼Œå¤„ç†weights_onlyé—®é¢˜
                try:
                    loaded_data = torch.load(model_config.file_path, map_location=self.device, weights_only=False)
                except TypeError:
                    # æ—§ç‰ˆæœ¬PyTorchä¸æ”¯æŒweights_onlyå‚æ•°
                    loaded_data = torch.load(model_config.file_path, map_location=self.device)
                
                # æ£€æŸ¥åŠ è½½çš„æ•°æ®ç±»å‹
                if isinstance(loaded_data, (dict, torch.nn.modules.container.OrderedDict)):
                    # å¦‚æœæ˜¯çŠ¶æ€å­—å…¸ï¼Œåˆ›å»ºDeepLabV3æ¨¡å‹å¹¶åŠ è½½æƒé‡
                    print(f"æ­£åœ¨ä¸º {model_config.name} åˆ›å»ºDeepLabV3æ¨¡å‹æ¶æ„...")
                    model = self._create_deeplabv3_model()
                    
                    # åŠ è½½çŠ¶æ€å­—å…¸
                    try:
                        model.load_state_dict(loaded_data, strict=False)
                        print(f"âœ… DeepLabV3æƒé‡åŠ è½½æˆåŠŸ")
                    except Exception as e:
                        print(f"è­¦å‘Š: DeepLabV3æƒé‡åŠ è½½éƒ¨åˆ†å¤±è´¥: {e}")
                    
                    model.to(self.device)
                    model.eval()
                    
                    # å¯ç”¨åŠç²¾åº¦æ¨ç†ï¼ˆå¦‚æœæ”¯æŒï¼‰
                    if self.device.type == 'cuda':
                        model.half()
                else:
                    model = loaded_data
                    if hasattr(model, 'eval'):
                        model.eval()
                    
                    # å¯ç”¨åŠç²¾åº¦æ¨ç†ï¼ˆå¦‚æœæ”¯æŒï¼‰
                    if self.device.type == 'cuda' and hasattr(model, 'half'):
                        model.half()
                    
            else:
                # é€šç”¨PyTorchæ¨¡å‹åŠ è½½ï¼Œå¤„ç†weights_onlyé—®é¢˜
                try:
                    model = torch.load(model_config.file_path, map_location=self.device, weights_only=False)
                except TypeError:
                    # æ—§ç‰ˆæœ¬PyTorchä¸æ”¯æŒweights_onlyå‚æ•°
                    model = torch.load(model_config.file_path, map_location=self.device)
                
                if hasattr(model, 'eval'):
                    model.eval()
                
                # å¯ç”¨åŠç²¾åº¦æ¨ç†ï¼ˆå¦‚æœæ”¯æŒï¼‰
                if self.device.type == 'cuda' and hasattr(model, 'half'):
                    model.half()
            
            # ç¼–è¯‘æ¨¡å‹ä»¥æé«˜æ¨ç†é€Ÿåº¦ï¼ˆPyTorch 2.0+ï¼‰
            # åªå¯¹å¯è°ƒç”¨çš„PyTorchæ¨¡å‹è¿›è¡Œç¼–è¯‘
            if (hasattr(torch, 'compile') and 
                not isinstance(model, YOLO) and 
                hasattr(model, '__call__') and 
                hasattr(model, 'parameters') and
                callable(model)):
                try:
                    model = torch.compile(model, mode='reduce-overhead')
                    print(f"âœ… æ°´é¢åˆ†å‰²æ¨¡å‹ç¼–è¯‘æˆåŠŸ: {model_config.name}")
                except Exception as e:
                    print(f"âš ï¸ æ°´é¢åˆ†å‰²æ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {e}")
            
            self.water_models[model_config.name] = model
            
        except Exception as e:
            raise ModelLoadError(model_config.file_path, str(e))
    
    def get_available_models(self) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return {
            'vehicle_models': list(self.vehicle_models.keys()),
            'water_models': list(self.water_models.keys())
        }
    
    def set_active_models(self, vehicle_model: str, water_model: str) -> bool:
        """è®¾ç½®å½“å‰æ´»è·ƒçš„æ¨¡å‹"""
        success = True
        
        if vehicle_model not in self.vehicle_models:
            print(f"è­¦å‘Š: è½¦è¾†æ£€æµ‹æ¨¡å‹ä¸å­˜åœ¨: {vehicle_model}")
            success = False
        else:
            self.current_vehicle_model = vehicle_model
        
        if water_model not in self.water_models:
            print(f"è­¦å‘Š: æ°´é¢åˆ†å‰²æ¨¡å‹ä¸å­˜åœ¨: {water_model}")
            success = False
        else:
            self.current_water_model = water_model
        
        return success
    
    def predict_vehicles(self, image: np.ndarray) -> List[Detection]:
        """ä½¿ç”¨å½“å‰è½¦è¾†æ£€æµ‹æ¨¡å‹è¿›è¡Œé¢„æµ‹ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.current_vehicle_model:
            raise InferenceError("è½¦è¾†æ£€æµ‹", "æœªè®¾ç½®æ´»è·ƒçš„è½¦è¾†æ£€æµ‹æ¨¡å‹")
        
        if self.current_vehicle_model not in self.vehicle_models:
            raise InferenceError("è½¦è¾†æ£€æµ‹", f"æ¨¡å‹ä¸å­˜åœ¨: {self.current_vehicle_model}")
        
        # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿æ¨ç†å®‰å…¨
        with self._inference_lock:
            try:
                model = self.vehicle_models[self.current_vehicle_model]
                
                # è·å–é…ç½®
                config = None
                for model_config in config_manager.config.vehicle_models:
                    if model_config.name == self.current_vehicle_model:
                        config = model_config
                        break
                
                if config is None:
                    raise InferenceError("è½¦è¾†æ£€æµ‹", f"æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: {self.current_vehicle_model}")
                
                # å†…å­˜ä¼˜åŒ–ï¼šåœ¨æ¨ç†å‰æ¸…ç†ç¼“å­˜
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œæ¨ç†
                if isinstance(model, YOLO) or "ultralytics" in str(type(model)).lower():
                    return self._predict_with_yolo_optimized(model, image, config)
                else:
                    return self._predict_with_pytorch_model_optimized(model, image, config, 'detection')
                    
            except Exception as e:
                raise InferenceError(self.current_vehicle_model, str(e))
    
    def predict_water(self, image: np.ndarray) -> np.ndarray:
        """ä½¿ç”¨å½“å‰æ°´é¢åˆ†å‰²æ¨¡å‹è¿›è¡Œé¢„æµ‹ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.current_water_model:
            raise InferenceError("æ°´é¢åˆ†å‰²", "æœªè®¾ç½®æ´»è·ƒçš„æ°´é¢åˆ†å‰²æ¨¡å‹")
        
        if self.current_water_model not in self.water_models:
            raise InferenceError("æ°´é¢åˆ†å‰²", f"æ¨¡å‹ä¸å­˜åœ¨: {self.current_water_model}")
        
        # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿æ¨ç†å®‰å…¨
        with self._inference_lock:
            try:
                model = self.water_models[self.current_water_model]
                
                # è·å–é…ç½®
                config = None
                for model_config in config_manager.config.water_models:
                    if model_config.name == self.current_water_model:
                        config = model_config
                        break
                
                if config is None:
                    raise InferenceError("æ°´é¢åˆ†å‰²", f"æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: {self.current_water_model}")
                
                # å†…å­˜ä¼˜åŒ–ï¼šåœ¨æ¨ç†å‰æ¸…ç†ç¼“å­˜
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œæ¨ç†
                if isinstance(model, YOLO) or "ultralytics" in str(type(model)).lower():
                    return self._predict_water_with_yolo_optimized(model, image, config)
                else:
                    return self._predict_water_with_pytorch_model_optimized(model, image, config)
                    
            except Exception as e:
                raise InferenceError(self.current_water_model, str(e))
    
    def _predict_with_yolo_optimized(self, model: YOLO, image: np.ndarray, config: ModelConfig) -> List[Detection]:
        """ä½¿ç”¨YOLOæ¨¡å‹è¿›è¡Œè½¦è¾†æ£€æµ‹ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        
        # ğŸ”¥ å›é€€ä¿®å¤ï¼šè®©YOLOä½¿ç”¨åŸç”Ÿé¢„å¤„ç†
        # é¿å…åŒé‡é¢„å¤„ç†é—®é¢˜ï¼Œè®©Ultralyticsè‡ªå·±å¤„ç†é¢„å¤„ç†
        
        # ä¼˜åŒ–çš„YOLOæ¨ç†è®¾ç½®
        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            results = model(
                image,  # ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œè®©YOLOè‡ªå·±é¢„å¤„ç†
                conf=config.confidence_threshold, 
                iou=config.nms_threshold,
                verbose=False,  # ç¦ç”¨è¯¦ç»†è¾“å‡º
                device=self.device,
                half=self.device.type == 'cuda'  # ä½¿ç”¨åŠç²¾åº¦ï¼ˆå¦‚æœæ˜¯GPUï¼‰
            )
        
        detections = []
        for result in results:
            if result.boxes is not None:
                # æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡
                boxes = result.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]
                confidences = result.boxes.conf.cpu().numpy()
                class_ids = result.boxes.cls.cpu().numpy().astype(int)
                
                # å‘é‡åŒ–å¤„ç†
                # å®šä¹‰æ·¹æ²¡ç­‰çº§æ˜ å°„ï¼ˆåŸºäºä½ çš„è®­ç»ƒæ•°æ®ï¼‰
                flood_level_mapping = {
                    0: 'cc',  # è½¦çª—åŠä»¥ä¸Š (ROOF_LEVEL)
                    1: 'cm',  # è½¦è½®é¡¶éƒ¨è‡³è½¦çª—ä¸‹æ²¿ (WINDOW_LEVEL)  
                    2: 'lt'   # è½¦è½®é¡¶éƒ¨åŠä»¥ä¸‹ (WHEEL_LEVEL)
                }
                
                for box, conf, cls_id in zip(boxes, confidences, class_ids):
                    bbox = BoundingBox(
                        x1=float(box[0]),
                        y1=float(box[1]),
                        x2=float(box[2]),
                        y2=float(box[3]),
                        confidence=float(conf)
                    )
                    
                    # ä½¿ç”¨æ·¹æ²¡ç­‰çº§ä½œä¸ºç±»åˆ«åç§°
                    flood_level_code = flood_level_mapping.get(int(cls_id), f'unknown_{cls_id}')
                    
                    detection = Detection(
                        bbox=bbox,
                        class_id=int(cls_id),
                        class_name=f"vehicle_{flood_level_code}"  # æ ‡è¯†ä¸ºè½¦è¾†+æ·¹æ²¡ç­‰çº§
                    )
                    detections.append(detection)
        
        return detections
    
    def _predict_with_yolo(self, model: YOLO, image: np.ndarray, config: ModelConfig) -> List[Detection]:
        """ä½¿ç”¨YOLOæ¨¡å‹è¿›è¡Œè½¦è¾†æ£€æµ‹ - å…¼å®¹æ€§æ–¹æ³•"""
        return self._predict_with_yolo_optimized(model, image, config)
    
    def _predict_with_pytorch_model_optimized(self, model, image: np.ndarray, config: ModelConfig, task: str) -> List[Detection]:
        """ä½¿ç”¨é€šç”¨PyTorchæ¨¡å‹è¿›è¡Œæ£€æµ‹ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯è°ƒç”¨
        if isinstance(model, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–æ¨¡å‹
            if 'model' in model:
                actual_model = model['model']
            elif 'state_dict' in model:
                raise InferenceError("æ¨¡å‹æ¨ç†", "æ£€æµ‹åˆ°state_dictæ ¼å¼ï¼Œéœ€è¦å…ˆåŠ è½½åˆ°æ¨¡å‹æ¶æ„ä¸­")
            else:
                raise InferenceError("æ¨¡å‹æ¨ç†", "ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼ï¼šå­—å…¸ç±»å‹ä½†æ— æ³•æ‰¾åˆ°æ¨¡å‹å¯¹è±¡")
        else:
            actual_model = model
        
        # ç¡®ä¿æ¨¡å‹å¯è°ƒç”¨
        if not callable(actual_model):
            raise InferenceError("æ¨¡å‹æ¨ç†", f"æ¨¡å‹ä¸å¯è°ƒç”¨ï¼Œç±»å‹: {type(actual_model)}")
        
        # ä¼˜åŒ–çš„é¢„å¤„ç†å›¾åƒ
        if len(image.shape) == 3:
            # ä½¿ç”¨æ›´å¿«çš„é¢„å¤„ç†æ–¹æ³•
            input_tensor = self._fast_preprocess(image, config.input_size).to(self.device, non_blocking=True)
        else:
            raise ValueError("è¾“å…¥å›¾åƒæ ¼å¼é”™è¯¯")
        
        # ä¼˜åŒ–çš„æ¨¡å‹æ¨ç†
        try:
            # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹æ­£ç¡®
            if hasattr(actual_model, 'parameters'):
                # è·å–æ¨¡å‹çš„æ•°æ®ç±»å‹
                model_dtype = next(actual_model.parameters()).dtype
                input_tensor = input_tensor.to(dtype=model_dtype)
            
            if self.device.type == 'cuda':
                with torch.no_grad(), torch.amp.autocast('cuda'):
                    outputs = actual_model(input_tensor)
            else:
                with torch.no_grad():
                    outputs = actual_model(input_tensor)
        except Exception as e:
            print(f"è­¦å‘Š: {config.name} æ¨¡å‹æ¨ç†å¤±è´¥: {e}ï¼Œè¿”å›ç©ºæ£€æµ‹ç»“æœ")
            return []
        
        # åå¤„ç†ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è¾“å‡ºæ ¼å¼è°ƒæ•´ï¼‰
        detections = []
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ¨¡å‹çš„è¾“å‡ºæ ¼å¼æ¥è§£æç»“æœ
        # å¯¹äºRT-DETRç­‰æ£€æµ‹æ¨¡å‹ï¼Œéœ€è¦æ ¹æ®å…·ä½“è¾“å‡ºæ ¼å¼è§£æ
        
        # ä¸´æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…æ¨ç†å¤±è´¥
        print(f"è­¦å‘Š: {config.name} æ¨¡å‹æ¨ç†æš‚æœªå®Œå…¨å®ç°ï¼Œè¿”å›ç©ºæ£€æµ‹ç»“æœ")
        
        return detections
    
    def _predict_with_pytorch_model(self, model, image: np.ndarray, config: ModelConfig, task: str) -> List[Detection]:
        """ä½¿ç”¨é€šç”¨PyTorchæ¨¡å‹è¿›è¡Œæ£€æµ‹ - å…¼å®¹æ€§æ–¹æ³•"""
        return self._predict_with_pytorch_model_optimized(model, image, config, task)
    
    def _predict_water_with_yolo_optimized(self, model: YOLO, image: np.ndarray, config: ModelConfig) -> np.ndarray:
        """ä½¿ç”¨YOLOåˆ†å‰²æ¨¡å‹è¿›è¡Œæ°´é¢åˆ†å‰² - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        
        # ğŸ”¥ å›é€€ä¿®å¤ï¼šè®©YOLOä½¿ç”¨åŸç”Ÿé¢„å¤„ç†
        # é¿å…åŒé‡é¢„å¤„ç†é—®é¢˜
        
        # ä¼˜åŒ–çš„YOLOåˆ†å‰²æ¨ç†
        with torch.no_grad():
            results = model(
                image,  # ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒ
                conf=config.confidence_threshold,
                verbose=False,
                device=self.device,
                half=self.device.type == 'cuda'
            )
        
        # åˆ›å»ºç©ºçš„æ©ç 
        mask = np.zeros(image.shape[:2], dtype=np.uint8)
        
        for result in results:
            if result.masks is not None:
                # è·å–åˆ†å‰²æ©ç  - æ‰¹é‡å¤„ç†
                masks = result.masks.data.cpu().numpy()
                
                # ä¼˜åŒ–çš„æ©ç å¤„ç†
                for seg_mask in masks:
                    # ä½¿ç”¨æ›´å¿«çš„æ’å€¼æ–¹æ³•è°ƒæ•´æ©ç å°ºå¯¸
                    if seg_mask.shape != image.shape[:2]:
                        resized_mask = cv2.resize(
                            seg_mask, 
                            (image.shape[1], image.shape[0]), 
                            interpolation=cv2.INTER_NEAREST  # æ›´å¿«çš„æ’å€¼æ–¹æ³•
                        )
                    else:
                        resized_mask = seg_mask
                    
                    # å‘é‡åŒ–æ“ä½œ
                    binary_mask = (resized_mask > 0.5).astype(np.uint8)
                    mask = np.maximum(mask, binary_mask)
        
        return mask
    
    def _predict_water_with_yolo(self, model: YOLO, image: np.ndarray, config: ModelConfig) -> np.ndarray:
        """ä½¿ç”¨YOLOåˆ†å‰²æ¨¡å‹è¿›è¡Œæ°´é¢åˆ†å‰² - å…¼å®¹æ€§æ–¹æ³•"""
        return self._predict_water_with_yolo_optimized(model, image, config)
    
    def _predict_water_with_pytorch_model_optimized(self, model, image: np.ndarray, config: ModelConfig) -> np.ndarray:
        """ä½¿ç”¨PyTorchåˆ†å‰²æ¨¡å‹è¿›è¡Œæ°´é¢åˆ†å‰² - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯è°ƒç”¨
        if isinstance(model, (dict, torch.nn.modules.container.OrderedDict)):
            # å¦‚æœæ˜¯å­—å…¸æˆ–OrderedDictæ ¼å¼ï¼Œå°è¯•æå–æ¨¡å‹
            if 'model' in model:
                actual_model = model['model']
            elif 'state_dict' in model:
                raise InferenceError("æ¨¡å‹æ¨ç†", "æ£€æµ‹åˆ°state_dictæ ¼å¼ï¼Œéœ€è¦å…ˆåŠ è½½åˆ°æ¨¡å‹æ¶æ„ä¸­")
            else:
                # å¯¹äºDeepLabV3ç­‰æ¨¡å‹ï¼Œå¯èƒ½ç›´æ¥æ˜¯OrderedDictçŠ¶æ€å­—å…¸
                print(f"è­¦å‘Š: {config.name} æ¨¡å‹ä¸ºçŠ¶æ€å­—å…¸æ ¼å¼ï¼Œæš‚æœªå®Œå…¨å®ç°ï¼Œè¿”å›ç©ºæ©ç ")
                return np.zeros(image.shape[:2], dtype=np.uint8)
        else:
            actual_model = model
        
        # ç¡®ä¿æ¨¡å‹å¯è°ƒç”¨
        if not callable(actual_model):
            print(f"è­¦å‘Š: {config.name} æ¨¡å‹ä¸å¯è°ƒç”¨ï¼Œç±»å‹: {type(actual_model)}ï¼Œè¿”å›ç©ºæ©ç ")
            return np.zeros(image.shape[:2], dtype=np.uint8)
        
        # ä¼˜åŒ–çš„é¢„å¤„ç†å›¾åƒ
        if len(image.shape) == 3:
            input_tensor = self._fast_preprocess(image, config.input_size, 'deeplabv3').to(self.device, non_blocking=True)
        else:
            raise ValueError("è¾“å…¥å›¾åƒæ ¼å¼é”™è¯¯")
        
        # ä¼˜åŒ–çš„æ¨¡å‹æ¨ç†
        try:
            if self.device.type == 'cuda':
                with torch.no_grad(), torch.amp.autocast('cuda'):
                    outputs = actual_model(input_tensor)
            else:
                with torch.no_grad():
                    outputs = actual_model(input_tensor)
        except Exception as e:
            print(f"è­¦å‘Š: {config.name} æ¨¡å‹æ¨ç†å¤±è´¥: {e}ï¼Œè¿”å›ç©ºæ©ç ")
            return np.zeros(image.shape[:2], dtype=np.uint8)
        
        # åå¤„ç†
        if isinstance(outputs, dict) and 'out' in outputs:
            # DeepLabV3è¾“å‡ºæ ¼å¼
            output = outputs['out']
        else:
            output = outputs
        
        # å¤„ç†å•ç±»åˆ«è¾“å‡º
        if output.shape[1] == 1:
            # å•ç±»åˆ«è¾“å‡ºï¼Œç›´æ¥ä½¿ç”¨sigmoidæ¿€æ´»
            pred_mask = torch.sigmoid(output).squeeze()
            
            # ç›´æ¥åœ¨GPUä¸Šè¿›è¡Œåå¤„ç†ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            if self.device.type == 'cuda':
                pred_mask = pred_mask.cpu()
            
            pred_mask = pred_mask.numpy()
            
            # ä½¿ç”¨é˜ˆå€¼è¿›è¡ŒäºŒå€¼åŒ–
            water_mask = (pred_mask > 0.5).astype(np.uint8)
        else:
            # å¤šç±»åˆ«è¾“å‡ºï¼Œä½¿ç”¨argmax
            pred_mask = torch.argmax(output, dim=1).squeeze()
            
            # ç›´æ¥åœ¨GPUä¸Šè¿›è¡Œåå¤„ç†ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            if self.device.type == 'cuda':
                pred_mask = pred_mask.cpu()
            
            pred_mask = pred_mask.numpy()
            
            # äºŒå€¼åŒ–ï¼ˆå‡è®¾æ°´é¢ç±»åˆ«ä¸º1ï¼‰
            water_mask = (pred_mask == 1).astype(np.uint8)
        
        # ä½¿ç”¨æ›´å¿«çš„æ’å€¼æ–¹æ³•è°ƒæ•´å°ºå¯¸åˆ°åŸå›¾å¤§å°
        if water_mask.shape != image.shape[:2]:
            water_mask = cv2.resize(
                water_mask.astype(np.uint8), 
                (image.shape[1], image.shape[0]),
                interpolation=cv2.INTER_NEAREST
            )
        
        return water_mask
    
    def _predict_water_with_pytorch_model(self, model, image: np.ndarray, config: ModelConfig) -> np.ndarray:
        """ä½¿ç”¨PyTorchåˆ†å‰²æ¨¡å‹è¿›è¡Œæ°´é¢åˆ†å‰² - å…¼å®¹æ€§æ–¹æ³•"""
        return self._predict_water_with_pytorch_model_optimized(model, image, config)
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        info = {
            'current_vehicle_model': self.current_vehicle_model,
            'current_water_model': self.current_water_model,
            'device': str(self.device),
            'available_models': self.get_available_models()
        }
        return info
    
    def unload_models(self):
        """å¸è½½æ‰€æœ‰æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜"""
        self.vehicle_models.clear()
        self.water_models.clear()
        self.current_vehicle_model = None
        self.current_water_model = None
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ¸…ç†ç¼“å­˜
        self._model_cache.clear()
        self._get_cached_preprocessing.cache_clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        print("æ‰€æœ‰æ¨¡å‹å·²å¸è½½")
    
    def _fast_preprocess(self, image: np.ndarray, input_size: Tuple[int, int], model_type: str = 'deeplabv3') -> torch.Tensor:
        """å¿«é€Ÿé¢„å¤„ç†æ–¹æ³• - æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨æ­£ç¡®çš„é¢„å¤„ç†"""
        # ä½¿ç”¨OpenCVè¿›è¡Œæ›´å¿«çš„é¢„å¤„ç†
        # è½¬æ¢ä¸ºRGB
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # å¿«é€Ÿç¼©æ”¾
        resized = cv2.resize(rgb_image, input_size, interpolation=cv2.INTER_LINEAR)
        
        # å½’ä¸€åŒ–
        normalized = resized.astype(np.float32) / 255.0
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ­£ç¡®çš„å½’ä¸€åŒ–æ–¹å¼
        if model_type.lower() in ['deeplabv3', 'deeplab']:
            # DeepLabV3è®­ç»ƒæ—¶ä½¿ç”¨çš„å½’ä¸€åŒ–ï¼šmean=[0,0,0], std=[1,1,1]
            # ç›¸å½“äºåªåš /255.0ï¼Œä¸åšImageNetæ ‡å‡†åŒ–
            pass  # å·²ç»åšäº† /255.0ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†
        else:
            # å…¶ä»–æ¨¡å‹å¯èƒ½éœ€è¦ImageNetæ ‡å‡†åŒ–
            mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
            std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
            normalized = (normalized - mean) / std
        
        # è½¬æ¢ä¸ºå¼ é‡
        tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)
        
        return tensor
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = {
            'models_loaded': len(self.vehicle_models) + len(self.water_models),
            'warmup_done': self._warmup_done
        }
        
        if torch.cuda.is_available():
            memory_info.update({
                'gpu_memory_allocated': torch.cuda.memory_allocated() / 1024**2,  # MB
                'gpu_memory_reserved': torch.cuda.memory_reserved() / 1024**2,   # MB
                'gpu_memory_cached': torch.cuda.memory_cached() / 1024**2        # MB
            })
        
        return memory_info
    
    def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # æ¸…ç†ç¼“å­˜
        if hasattr(self, '_model_cache'):
            self._model_cache.clear()
        
        if hasattr(self, '_get_cached_preprocessing'):
            self._get_cached_preprocessing.cache_clear()
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        print("å†…å­˜ä¼˜åŒ–å®Œæˆ")
    
    def get_available_models(self) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return {
            'vehicle_models': list(self.vehicle_models.keys()),
            'water_models': list(self.water_models.keys())
        }
    
    def set_active_models(self, vehicle_model: str, water_model: str) -> bool:
        """è®¾ç½®æ´»è·ƒæ¨¡å‹"""
        success = True
        
        if vehicle_model and vehicle_model in self.vehicle_models:
            self.current_vehicle_model = vehicle_model
        else:
            print(f"è½¦è¾†æ£€æµ‹æ¨¡å‹ä¸å­˜åœ¨: {vehicle_model}")
            success = False
        
        if water_model and water_model in self.water_models:
            self.current_water_model = water_model
        else:
            print(f"æ°´é¢åˆ†å‰²æ¨¡å‹ä¸å­˜åœ¨: {water_model}")
            success = False
        
        return success
    
    def get_current_models(self) -> Dict[str, Optional[str]]:
        """è·å–å½“å‰æ´»è·ƒçš„æ¨¡å‹"""
        return {
            'vehicle_model': self.current_vehicle_model,
            'water_model': self.current_water_model
        }
    
    def _create_deeplabv3_model(self):
        """åˆ›å»ºDeepLabV3æ¨¡å‹æ¶æ„"""
        try:
            import torchvision.models.segmentation as segmentation
            
            # åˆ›å»ºDeepLabV3æ¨¡å‹ï¼ˆä½¿ç”¨ResNet101ä½œä¸ºbackboneï¼‰
            # ä½¿ç”¨1ä¸ªç±»åˆ«ï¼Œå› ä¸ºåŸå§‹æ¨¡å‹æ˜¯å•ç±»åˆ«çš„
            model = segmentation.deeplabv3_resnet101(
                weights=None,  # ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
                num_classes=1   # 1ä¸ªç±»åˆ«ï¼šæ°´é¢ï¼ˆèƒŒæ™¯é€šè¿‡é˜ˆå€¼å¤„ç†ï¼‰
            )
            
            return model
            
        except Exception as e:
            print(f"åˆ›å»ºDeepLabV3æ¨¡å‹å¤±è´¥: {e}")
            # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œè¿”å›ç®€å•çš„æ°´é¢æ£€æµ‹å™¨
            return self._create_simple_water_detector()
    
    def _create_simple_water_detector(self):
        """åˆ›å»ºä¸€ä¸ªç®€å•çš„æ°´é¢æ£€æµ‹å™¨ï¼ˆå½“DeepLabV3ä¸å¯ç”¨æ—¶ï¼‰"""
        class SimpleWaterDetector:
            def __init__(self, device):
                self.device = device
            
            def __call__(self, input_tensor):
                # ç®€å•çš„åŸºäºé¢œè‰²çš„æ°´é¢æ£€æµ‹
                # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
                batch_size, channels, height, width = input_tensor.shape
                
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ©ç ï¼šæ£€æµ‹è“è‰²åŒºåŸŸä½œä¸ºæ°´é¢
                # å°†è¾“å…¥ä»[-1,1]èŒƒå›´è½¬æ¢å›[0,1]
                normalized_input = (input_tensor + 1) / 2
                
                # æå–è“è‰²é€šé“ï¼ˆå‡è®¾è¾“å…¥æ˜¯RGBæ ¼å¼ï¼‰
                blue_channel = normalized_input[:, 2, :, :]  # è“è‰²é€šé“
                green_channel = normalized_input[:, 1, :, :]  # ç»¿è‰²é€šé“
                red_channel = normalized_input[:, 0, :, :]   # çº¢è‰²é€šé“
                
                # ç®€å•çš„è“è‰²æ£€æµ‹ï¼šè“è‰² > çº¢è‰² ä¸” è“è‰² > ç»¿è‰²
                water_mask = (blue_channel > red_channel) & (blue_channel > green_channel) & (blue_channel > 0.3)
                
                # åˆ›å»ºè¾“å‡ºæ ¼å¼ï¼ˆæ¨¡æ‹ŸDeepLabV3çš„è¾“å‡ºï¼‰
                output = torch.zeros(batch_size, 2, height, width, device=input_tensor.device)
                output[:, 0, :, :] = ~water_mask  # èƒŒæ™¯
                output[:, 1, :, :] = water_mask   # æ°´é¢
                
                return {'out': output}
        
        return SimpleWaterDetector(self.device)
    

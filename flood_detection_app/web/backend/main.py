"""
FastAPIåç«¯æœåŠ¡ - å®Œæ•´ç‰ˆ
FastAPI Backend Service for Flood Detection Web Application - Complete Version
"""

import os
import io
import base64
import time
import asyncio
import hashlib
import json
import psutil
from typing import List, Dict, Any, Optional, Union
from fastapi import FastAPI, UploadFile, File, HTTPException, Form, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
import numpy as np
from PIL import Image
import cv2
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from functools import lru_cache
import gc
import logging
from datetime import datetime, timedelta

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from ...core import (
    ModelManager, ImageProcessor, FloodAnalyzer, VisualizationEngine,
    config_manager
)
from ...core.exceptions import FloodDetectionError, ModelLoadError, InferenceError
from ...core.data_models import Statistics, AnalysisResult


# Pydanticæ¨¡å‹å®šä¹‰
class VehicleResult(BaseModel):
    id: int
    bbox: List[float]  # [x1, y1, x2, y2]
    confidence: float
    flood_level: str
    overlap_ratio: float


class AnalysisResponse(BaseModel):
    success: bool
    message: str
    vehicles: List[VehicleResult]
    statistics: Dict[str, Any]
    processing_time: float
    result_image_base64: str
    water_coverage_percentage: float
    cache_hit: Optional[bool] = False
    analysis_id: Optional[str] = None


class BatchAnalysisRequest(BaseModel):
    vehicle_model: str
    water_model: str
    task_mode: str = "combined"  # "vehicle_only", "water_only", "combined"


class BatchAnalysisResponse(BaseModel):
    success: bool
    message: str
    total_files: int
    processed_files: int
    failed_files: int
    results: List[Dict[str, Any]]
    total_processing_time: float
    batch_id: str


class ModelsResponse(BaseModel):
    vehicle_models: List[str]
    water_models: List[str]


class HealthResponse(BaseModel):
    status: str
    timestamp: float
    models_loaded: bool
    version: str


class PerformanceResponse(BaseModel):
    cpu_usage: float
    memory_usage: float
    memory_available: float
    cache_size: int
    active_models: Dict[str, bool]
    uptime: float


class ErrorResponse(BaseModel):
    success: bool
    error: str
    error_code: str


class CacheInfo(BaseModel):
    total_entries: int
    memory_usage_mb: float
    hit_rate: float
    oldest_entry: Optional[str] = None


# åˆ›å»ºFastAPIåº”ç”¨ - å®Œæ•´ç‰ˆ
app = FastAPI(
    title="ç§¯æ°´è½¦è¾†æ£€æµ‹API - å®Œæ•´ç‰ˆ",
    description="åŸºäºæ·±åº¦å­¦ä¹ çš„ç§¯æ°´è½¦è¾†æ£€æµ‹åˆ†ææœåŠ¡ - åŠŸèƒ½å®Œæ•´ç‰ˆ",
    version="2.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
model_manager = None
image_processor = None
flood_analyzer = None
viz_engine = None
models_loaded = False
app_start_time = time.time()

# æ€§èƒ½ä¼˜åŒ–è®¾ç½®
executor = ThreadPoolExecutor(max_workers=6)  # å¢åŠ çº¿ç¨‹æ± å¤§å°
analysis_lock = threading.Lock()  # é˜²æ­¢å¹¶å‘åˆ†æå†²çª
cache_lock = threading.Lock()

# æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
class AnalysisCache:
    def __init__(self, max_size: int = 100, ttl_hours: int = 24):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl = timedelta(hours=ttl_hours)
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, image_data: bytes, vehicle_model: str, water_model: str, task_mode: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = image_data + vehicle_model.encode() + water_model.encode() + task_mode.encode()
        return hashlib.md5(content).hexdigest()
    
    def get(self, key: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜"""
        with cache_lock:
            if key in self.cache:
                entry = self.cache[key]
                # æ£€æŸ¥TTL
                if datetime.now() - entry['timestamp'] < self.ttl:
                    self.access_times[key] = datetime.now()
                    self.hits += 1
                    return entry['data']
                else:
                    # è¿‡æœŸåˆ é™¤
                    del self.cache[key]
                    if key in self.access_times:
                        del self.access_times[key]
            
            self.misses += 1
            return None
    
    def set(self, key: str, data: Dict):
        """è®¾ç½®ç¼“å­˜"""
        with cache_lock:
            # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
            if len(self.cache) >= self.max_size:
                oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
                del self.cache[oldest_key]
                del self.access_times[oldest_key]
            
            self.cache[key] = {
                'data': data,
                'timestamp': datetime.now()
            }
            self.access_times[key] = datetime.now()
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with cache_lock:
            self.cache.clear()
            self.access_times.clear()
            self.hits = 0
            self.misses = 0
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        with cache_lock:
            total_requests = self.hits + self.misses
            hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
            memory_usage = 0
            for entry in self.cache.values():
                memory_usage += len(str(entry).encode('utf-8'))
            
            oldest_entry = None
            if self.access_times:
                oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
                oldest_entry = self.access_times[oldest_key].isoformat()
            
            return {
                'total_entries': len(self.cache),
                'memory_usage_mb': memory_usage / (1024 * 1024),
                'hit_rate': hit_rate,
                'oldest_entry': oldest_entry
            }

# åˆ›å»ºç¼“å­˜å®ä¾‹
analysis_cache = AnalysisCache()


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global model_manager, image_processor, flood_analyzer, viz_engine, models_loaded
    
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨...")
        model_manager = ModelManager()
        
        logger.info("æ­£åœ¨åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨...")
        image_processor = ImageProcessor()
        
        logger.info("æ­£åœ¨åˆå§‹åŒ–æ´ªæ°´åˆ†æå™¨...")
        flood_analyzer = FloodAnalyzer()
        
        logger.info("æ­£åœ¨åˆå§‹åŒ–å¯è§†åŒ–å¼•æ“...")
        viz_engine = VisualizationEngine()
        
        models_loaded = True
        logger.info("æ‰€æœ‰æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        models_loaded = False


@app.get("/", response_model=Dict[str, str])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "ç§¯æ°´è½¦è¾†æ£€æµ‹API - å®Œæ•´ç‰ˆ",
        "version": "2.0.0",
        "status": "running" if models_loaded else "initializing"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return HealthResponse(
        status="healthy" if models_loaded else "initializing",
        timestamp=time.time(),
        models_loaded=models_loaded,
        version="2.0.0"
    )


@app.get("/models", response_model=ModelsResponse)
async def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    if not models_loaded:
        raise HTTPException(status_code=503, detail="Models not loaded yet")
    
    try:
        vehicle_models = model_manager.get_available_vehicle_models()
        water_models = model_manager.get_available_water_models()
        
        return ModelsResponse(
            vehicle_models=vehicle_models,
            water_models=water_models
        )
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get models: {str(e)}")


@app.get("/performance", response_model=PerformanceResponse)
async def get_performance_metrics():
    """è·å–æ€§èƒ½æŒ‡æ ‡"""
    try:
        # è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # è·å–æ¨¡å‹çŠ¶æ€
        active_models = {}
        if models_loaded and model_manager:
            try:
                active_models = {
                    "vehicle_model_loaded": model_manager.vehicle_model is not None,
                    "water_model_loaded": model_manager.water_model is not None
                }
            except:
                active_models = {"vehicle_model_loaded": False, "water_model_loaded": False}
        
        # è®¡ç®—è¿è¡Œæ—¶é—´
        uptime = time.time() - app_start_time
        
        return PerformanceResponse(
            cpu_usage=cpu_percent,
            memory_usage=memory.percent,
            memory_available=memory.available / (1024**3),  # GB
            cache_size=len(analysis_cache.cache),
            active_models=active_models,
            uptime=uptime
        )
    except Exception as e:
        logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get performance metrics: {str(e)}")


@app.post("/optimize")
async def optimize_performance():
    """æ€§èƒ½ä¼˜åŒ–"""
    try:
        # æ¸…ç†ç¼“å­˜
        analysis_cache.clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # å¦‚æœæœ‰CUDAï¼Œæ¸…ç†GPUç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except ImportError:
            pass
        
        return {"success": True, "message": "æ€§èƒ½ä¼˜åŒ–å®Œæˆ"}
    except Exception as e:
        logger.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Optimization failed: {str(e)}")


@app.get("/cache/info", response_model=CacheInfo)
async def get_cache_info():
    """è·å–ç¼“å­˜ä¿¡æ¯"""
    try:
        stats = analysis_cache.get_stats()
        return CacheInfo(**stats)
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get cache info: {str(e)}")


@app.delete("/cache")
async def clear_cache():
    """æ¸…ç©ºç¼“å­˜"""
    try:
        analysis_cache.clear()
        return {"success": True, "message": "ç¼“å­˜å·²æ¸…ç©º"}
    except Exception as e:
        logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")


def process_single_image(image_data: bytes, vehicle_model: str, water_model: str, task_mode: str) -> Dict[str, Any]:
    """å¤„ç†å•å¼ å›¾åƒ"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = analysis_cache._generate_key(image_data, vehicle_model, water_model, task_mode)
        cached_result = analysis_cache.get(cache_key)
        if cached_result:
            cached_result['cache_hit'] = True
            return cached_result
        
        # è§£ç å›¾åƒ
        image = Image.open(io.BytesIO(image_data))
        image_np = np.array(image)
        
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
        if len(image_np.shape) == 3 and image_np.shape[2] == 4:  # RGBA
            image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)
        elif len(image_np.shape) == 3 and image_np.shape[2] == 3:  # RGB
            pass  # å·²ç»æ˜¯RGB
        else:
            raise ValueError("Unsupported image format")
        
        start_time = time.time()
        
        with analysis_lock:
            # æ ¹æ®ä»»åŠ¡æ¨¡å¼è®¾ç½®æ¨¡å‹
            if task_mode == "vehicle_only":
                model_manager.set_active_models(vehicle_model, None)
            elif task_mode == "water_only":
                model_manager.set_active_models(None, water_model)
            else:  # combined
                model_manager.set_active_models(vehicle_model, water_model)
            
            # æ‰§è¡Œåˆ†æ
            vehicles = []
            water_mask = None
            
            if task_mode in ["vehicle_only", "combined"]:
                vehicles = model_manager.predict_vehicles(image_np)
            
            if task_mode in ["water_only", "combined"]:
                water_mask = model_manager.predict_water(image_np)
            
            # åˆ†æç»“æœ
            if task_mode == "combined" and vehicles and water_mask is not None:
                analysis_result = flood_analyzer.analyze_scene(vehicles, water_mask)
            else:
                # åˆ›å»ºç®€åŒ–çš„åˆ†æç»“æœ
                from ...core.data_models import Statistics
                
                if water_mask is not None:
                    water_coverage = (np.sum(water_mask > 0) / water_mask.size) * 100
                else:
                    water_coverage = 0.0
                
                stats = Statistics(
                    total_vehicles=len(vehicles) if vehicles else 0,
                    light_flood_count=0,
                    moderate_flood_count=0,
                    severe_flood_count=0,
                    water_coverage_percentage=water_coverage,
                    processing_time=time.time() - start_time
                )
                
                analysis_result = AnalysisResult(
                    vehicles=vehicles if vehicles else [],
                    statistics=stats,
                    water_mask=water_mask
                )
            
            # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
            result_image = viz_engine.create_visualization(
                image_np, analysis_result.vehicles, analysis_result.water_mask
            )
            
            # è½¬æ¢ä¸ºbase64
            result_pil = Image.fromarray(result_image)
            buffer = io.BytesIO()
            result_pil.save(buffer, format='PNG')
            result_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        processing_time = time.time() - start_time
        
        # æ„å»ºå“åº”
        response_data = {
            "success": True,
            "message": "åˆ†æå®Œæˆ",
            "vehicles": [
                {
                    "id": i,
                    "bbox": vehicle.bbox.tolist() if hasattr(vehicle.bbox, 'tolist') else list(vehicle.bbox),
                    "confidence": float(vehicle.confidence),
                    "flood_level": vehicle.flood_level if hasattr(vehicle, 'flood_level') else "unknown",
                    "overlap_ratio": float(vehicle.overlap_ratio) if hasattr(vehicle, 'overlap_ratio') else 0.0
                }
                for i, vehicle in enumerate(analysis_result.vehicles)
            ],
            "statistics": {
                "total_vehicles": analysis_result.statistics.total_vehicles,
                "light_flood_count": analysis_result.statistics.light_flood_count,
                "moderate_flood_count": analysis_result.statistics.moderate_flood_count,
                "severe_flood_count": analysis_result.statistics.severe_flood_count,
                "water_coverage_percentage": analysis_result.statistics.water_coverage_percentage,
                "processing_time": processing_time
            },
            "processing_time": processing_time,
            "result_image_base64": result_base64,
            "water_coverage_percentage": analysis_result.statistics.water_coverage_percentage,
            "cache_hit": False,
            "analysis_id": cache_key
        }
        
        # ç¼“å­˜ç»“æœ
        analysis_cache.set(cache_key, response_data)
        
        return response_data
        
    except Exception as e:
        logger.error(f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
        raise Exception(f"Image processing failed: {str(e)}")


@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_image(
    file: UploadFile = File(...),
    vehicle_model: str = Form(...),
    water_model: str = Form(...),
    task_mode: str = Form(default="combined")
):
    """åˆ†æå•å¼ å›¾åƒ"""
    if not models_loaded:
        raise HTTPException(status_code=503, detail="Models not loaded yet")
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.content_type or not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="Invalid file type. Please upload an image.")
    
    try:
        # è¯»å–å›¾åƒæ•°æ®
        image_data = await file.read()
        
        # éªŒè¯æ–‡ä»¶å¤§å° (æœ€å¤§50MB)
        if len(image_data) > 50 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large. Maximum size is 50MB.")
        
        # åœ¨çº¿ç¨‹æ± ä¸­å¤„ç†å›¾åƒ
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            executor, 
            process_single_image, 
            image_data, 
            vehicle_model, 
            water_model, 
            task_mode
        )
        
        return AnalysisResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


@app.post("/analyze/batch", response_model=BatchAnalysisResponse)
async def analyze_batch(
    files: List[UploadFile] = File(...),
    vehicle_model: str = Form(...),
    water_model: str = Form(...),
    task_mode: str = Form(default="combined")
):
    """æ‰¹é‡åˆ†æå›¾åƒ"""
    if not models_loaded:
        raise HTTPException(status_code=503, detail="Models not loaded yet")
    
    # éªŒè¯æ–‡ä»¶æ•°é‡
    if len(files) > 50:
        raise HTTPException(status_code=400, detail="Too many files. Maximum is 50 files per batch.")
    
    batch_id = hashlib.md5(f"{time.time()}_{len(files)}".encode()).hexdigest()
    start_time = time.time()
    
    results = []
    processed_files = 0
    failed_files = 0
    
    try:
        # è¯»å–æ‰€æœ‰æ–‡ä»¶æ•°æ®
        file_data_list = []
        for i, file in enumerate(files):
            if not file.content_type or not file.content_type.startswith('image/'):
                failed_files += 1
                results.append({
                    "filename": file.filename,
                    "success": False,
                    "error": "Invalid file type",
                    "index": i
                })
                continue
            
            try:
                data = await file.read()
                if len(data) > 50 * 1024 * 1024:
                    failed_files += 1
                    results.append({
                        "filename": file.filename,
                        "success": False,
                        "error": "File too large",
                        "index": i
                    })
                    continue
                
                file_data_list.append((i, file.filename, data))
            except Exception as e:
                failed_files += 1
                results.append({
                    "filename": file.filename,
                    "success": False,
                    "error": str(e),
                    "index": i
                })
        
        # å¹¶è¡Œå¤„ç†å›¾åƒ
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for index, filename, data in file_data_list:
            task = loop.run_in_executor(
                executor,
                process_single_image,
                data,
                vehicle_model,
                water_model,
                task_mode
            )
            tasks.append((index, filename, task))
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for index, filename, task in tasks:
            try:
                result = await task
                result['filename'] = filename
                result['index'] = index
                results.append(result)
                processed_files += 1
            except Exception as e:
                failed_files += 1
                results.append({
                    "filename": filename,
                    "success": False,
                    "error": str(e),
                    "index": index
                })
        
        total_processing_time = time.time() - start_time
        
        return BatchAnalysisResponse(
            success=True,
            message=f"æ‰¹é‡åˆ†æå®Œæˆ: {processed_files}æˆåŠŸ, {failed_files}å¤±è´¥",
            total_files=len(files),
            processed_files=processed_files,
            failed_files=failed_files,
            results=results,
            total_processing_time=total_processing_time,
            batch_id=batch_id
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Batch analysis failed: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    global model_manager, image_processor, flood_analyzer, viz_engine, models_loaded
    
    try:
        print("æ­£åœ¨åˆå§‹åŒ–åç«¯æœåŠ¡...")
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        model_manager = ModelManager()
        image_processor = ImageProcessor()
        flood_analyzer = FloodAnalyzer()
        viz_engine = VisualizationEngine()
        
        # åŠ è½½æ¨¡å‹
        print("æ­£åœ¨åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹...")
        success = model_manager.load_models()
        
        if success:
            models_loaded = True
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print("âš ï¸ éƒ¨åˆ†æ¨¡å‹åŠ è½½å¤±è´¥")
            
        print("ğŸš€ åç«¯æœåŠ¡å¯åŠ¨å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ åç«¯æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        models_loaded = False


@app.get("/", response_model=Dict[str, str])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "ç§¯æ°´è½¦è¾†æ£€æµ‹APIæœåŠ¡",
        "version": "1.0.0",
        "docs": "/docs"
    }


@app.get("/api/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return HealthResponse(
        status="healthy" if models_loaded else "degraded",
        timestamp=time.time(),
        models_loaded=models_loaded,
        version="1.0.0"
    )


@app.get("/api/models", response_model=ModelsResponse)
async def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        if not models_loaded or model_manager is None:
            raise HTTPException(
                status_code=503,
                detail="æ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨"
            )
        
        available_models = model_manager.get_available_models()
        
        return ModelsResponse(
            vehicle_models=available_models.get('vehicle_models', []),
            water_models=available_models.get('water_models', [])
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}"
        )


@app.post("/api/analyze", response_model=AnalysisResponse)
async def analyze_image(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    vehicle_model: str = Form("YOLOv11 Car Detection"),
    water_model: str = Form("DeepLabV3 Water Segmentation")
):
    """å•å¼ å›¾åƒåˆ†ææ¥å£"""
    """å›¾åƒåˆ†ææ¥å£ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        if not models_loaded or model_manager is None:
            raise HTTPException(
                status_code=503,
                detail="æ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨"
            )
        
        # éªŒè¯æ–‡ä»¶ç±»å‹å’Œå¤§å°
        if not file.content_type.startswith('image/'):
            raise HTTPException(
                status_code=400,
                detail="æ–‡ä»¶ç±»å‹é”™è¯¯ï¼Œè¯·ä¸Šä¼ å›¾åƒæ–‡ä»¶"
            )
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶ä¸º10MBï¼‰
        if file.size and file.size > 10 * 1024 * 1024:
            raise HTTPException(
                status_code=400,
                detail="æ–‡ä»¶è¿‡å¤§ï¼Œè¯·ä¸Šä¼ å°äº10MBçš„å›¾åƒ"
            )
        
        start_time = time.time()
        
        # å¼‚æ­¥è¯»å–å›¾åƒæ–‡ä»¶
        image_data = await file.read()
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{vehicle_model}_{water_model}_{hash(image_data)}"
        
        # æ£€æŸ¥ç¼“å­˜
        with cache_lock:
            if cache_key in request_cache:
                cached_result = request_cache[cache_key]
                print(f"ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œç¼“å­˜é”®: {cache_key[:20]}...")
                return cached_result
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒCPUå¯†é›†å‹ä»»åŠ¡
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            executor, 
            perform_analysis_optimized, 
            image_data, 
            vehicle_model, 
            water_model
        )
        
        # æ·»åŠ åå°ä»»åŠ¡è¿›è¡Œå†…å­˜æ¸…ç†
        background_tasks.add_task(cleanup_memory)
        
        # ç¼“å­˜ç»“æœï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
        with cache_lock:
            if len(request_cache) > 50:  # é™åˆ¶ç¼“å­˜æ¡ç›®æ•°
                # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(request_cache))
                del request_cache[oldest_key]
            request_cache[cache_key] = result
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"åˆ†æå¤±è´¥: {str(e)}"
        )


def perform_analysis_optimized(
    image_data: bytes, 
    vehicle_model: str, 
    water_model: str
) -> AnalysisResponse:
    """
    ä¼˜åŒ–çš„å›¾åƒåˆ†æå‡½æ•°
    
    Args:
        image_data: å›¾åƒå­—èŠ‚æ•°æ®
        vehicle_model: è½¦è¾†æ£€æµ‹æ¨¡å‹åç§°
        water_model: æ°´é¢åˆ†å‰²æ¨¡å‹åç§°
        
    Returns:
        AnalysisResponse: åˆ†æå“åº”
    """
    start_time = time.time()
    
    try:
        # ä½¿ç”¨åˆ†æé”é˜²æ­¢å¹¶å‘å†²çª
        with analysis_lock:
            # å¿«é€ŸåŠ è½½å›¾åƒ
            image = load_image_from_bytes_fast(image_data)
            
            # è®¾ç½®æ¨¡å‹
            success = model_manager.set_active_models(vehicle_model, water_model)
            if not success:
                raise ValueError(f"æ— æ³•è®¾ç½®æ¨¡å‹: {vehicle_model}, {water_model}")
            
            # æ‰§è¡Œåˆ†æ
            analysis_result = perform_analysis_fast(image)
            
            # ç”Ÿæˆç»“æœå›¾åƒ
            result_image = viz_engine.create_result_image_fast(image, analysis_result)
            result_image_base64 = image_to_base64_fast(result_image)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            
            # æ„å»ºè½¦è¾†ç»“æœ
            vehicles = []
            for vehicle in analysis_result.vehicles:
                vehicles.append(VehicleResult(
                    id=vehicle.vehicle_id,
                    bbox=[
                        vehicle.detection.bbox.x1,
                        vehicle.detection.bbox.y1,
                        vehicle.detection.bbox.x2,
                        vehicle.detection.bbox.y2
                    ],
                    confidence=vehicle.detection.bbox.confidence,
                    flood_level=vehicle.flood_level.value,
                    overlap_ratio=vehicle.overlap_ratio
                ))
            
            # æ„å»ºç»Ÿè®¡ä¿¡æ¯
            stats = analysis_result.statistics
            statistics = {
                "total_vehicles": stats.total_vehicles,
                "light_flood_count": stats.light_flood_count,
                "moderate_flood_count": stats.moderate_flood_count,
                "severe_flood_count": stats.severe_flood_count,
                "water_coverage_percentage": stats.water_coverage_percentage,
                "processing_time": processing_time
            }
            
            return AnalysisResponse(
                success=True,
                message="åˆ†æå®Œæˆ",
                vehicles=vehicles,
                statistics=statistics,
                processing_time=processing_time,
                result_image_base64=result_image_base64,
                water_coverage_percentage=stats.water_coverage_percentage
            )
    
    except Exception as e:
        raise ValueError(f"åˆ†æå¤±è´¥: {str(e)}")

def load_image_from_bytes_fast(image_data: bytes) -> np.ndarray:
    """å¿«é€Ÿä»å­—èŠ‚æ•°æ®åŠ è½½å›¾åƒ"""
    try:
        # ä½¿ç”¨æ›´å¿«çš„å›¾åƒåŠ è½½æ–¹æ³•
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            # å›é€€åˆ°PILæ–¹æ³•
            pil_image = Image.open(io.BytesIO(image_data))
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        return image
        
    except Exception as e:
        raise ValueError(f"å›¾åƒåŠ è½½å¤±è´¥: {str(e)}")

def load_image_from_bytes(image_data: bytes) -> np.ndarray:
    """ä»å­—èŠ‚æ•°æ®åŠ è½½å›¾åƒ - å…¼å®¹æ€§æ–¹æ³•"""
    return load_image_from_bytes_fast(image_data)


def perform_analysis_fast(image: np.ndarray) -> AnalysisResult:
    """æ‰§è¡Œå¿«é€Ÿå›¾åƒåˆ†æ"""
    try:
        # è½¦è¾†æ£€æµ‹
        vehicles = model_manager.predict_vehicles(image)
        
        # æ°´é¢åˆ†å‰²
        water_mask = model_manager.predict_water(image)
        
        # æ·¹æ²¡åˆ†æ - ä½¿ç”¨æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬
        analysis_result = flood_analyzer.analyze_scene_batch(vehicles, water_mask)
        
        return analysis_result
        
    except Exception as e:
        raise InferenceError("å›¾åƒåˆ†æ", str(e))

def perform_analysis(image: np.ndarray) -> AnalysisResult:
    """æ‰§è¡Œå›¾åƒåˆ†æ - å…¼å®¹æ€§æ–¹æ³•"""
    return perform_analysis_fast(image)


def image_to_base64_fast(image: np.ndarray) -> str:
    """å¿«é€Ÿå°†å›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    try:
        # ä½¿ç”¨OpenCVç›´æ¥ç¼–ç ï¼Œé¿å…PILè½¬æ¢
        success, buffer = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 85])
        
        if not success:
            raise ValueError("å›¾åƒç¼–ç å¤±è´¥")
        
        # ç¼–ç ä¸ºbase64
        image_base64 = base64.b64encode(buffer).decode('utf-8')
        
        return image_base64
        
    except Exception as e:
        raise ValueError(f"å›¾åƒç¼–ç å¤±è´¥: {str(e)}")

def image_to_base64(image: np.ndarray) -> str:
    """å°†å›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸² - å…¼å®¹æ€§æ–¹æ³•"""
    return image_to_base64_fast(image)


# é”™è¯¯å¤„ç†
@app.exception_handler(FloodDetectionError)
async def flood_detection_error_handler(request, exc: FloodDetectionError):
    """å¤„ç†è‡ªå®šä¹‰å¼‚å¸¸"""
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            success=False,
            error=exc.message,
            error_code=exc.error_code or "UNKNOWN_ERROR"
        ).dict()
    )


@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc: HTTPException):
    """å¤„ç†HTTPå¼‚å¸¸"""
    return JSONResponse(
        status_code=exc.status_code,
        content=ErrorResponse(
            success=False,
            error=exc.detail,
            error_code=f"HTTP_{exc.status_code}"
        ).dict()
    )


def cleanup_memory():
    """åå°å†…å­˜æ¸…ç†ä»»åŠ¡"""
    try:
        # æ¸…ç†æ¨¡å‹ç®¡ç†å™¨ç¼“å­˜
        if model_manager:
            model_manager.optimize_memory()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        print("åå°å†…å­˜æ¸…ç†å®Œæˆ")
    except Exception as e:
        print(f"å†…å­˜æ¸…ç†å¤±è´¥: {e}")

@app.get("/api/performance")
async def get_performance_info():
    """è·å–æ€§èƒ½ä¿¡æ¯"""
    try:
        performance_info = {
            "models_loaded": models_loaded,
            "cache_size": len(request_cache),
            "executor_threads": executor._max_workers
        }
        
        if model_manager:
            performance_info.update(model_manager.get_memory_usage())
        
        return performance_info
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ€§èƒ½ä¿¡æ¯å¤±è´¥: {str(e)}"
        )

@app.post("/api/batch-analyze")
async def batch_analyze_images(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    vehicle_model: str = Form("YOLOv11 Car Detection"),
    water_model: str = Form("DeepLabV3 Water Segmentation")
):
    """æ‰¹é‡å›¾åƒåˆ†ææ¥å£"""
    try:
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        if not models_loaded or model_manager is None:
            raise HTTPException(
                status_code=503,
                detail="æ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨"
            )
        
        # é™åˆ¶æ‰¹é‡å¤§å°
        if len(files) > 20:
            raise HTTPException(
                status_code=400,
                detail="æ‰¹é‡åˆ†ææœ€å¤šæ”¯æŒ20ä¸ªæ–‡ä»¶"
            )
        
        results = []
        start_time = time.time()
        
        for i, file in enumerate(files):
            try:
                # éªŒè¯æ–‡ä»¶
                if not file.content_type.startswith('image/'):
                    results.append({
                        "filename": file.filename,
                        "success": False,
                        "error": "æ–‡ä»¶ç±»å‹é”™è¯¯",
                        "result": None
                    })
                    continue
                
                if file.size and file.size > 10 * 1024 * 1024:
                    results.append({
                        "filename": file.filename,
                        "success": False,
                        "error": "æ–‡ä»¶è¿‡å¤§",
                        "result": None
                    })
                    continue
                
                # åˆ†æå›¾åƒ
                image_data = await file.read()
                
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œåˆ†æ
                loop = asyncio.get_event_loop()
                analysis_result = await loop.run_in_executor(
                    executor,
                    perform_analysis_optimized,
                    image_data,
                    vehicle_model,
                    water_model
                )
                
                results.append({
                    "filename": file.filename,
                    "success": True,
                    "error": None,
                    "result": analysis_result
                })
                
            except Exception as e:
                results.append({
                    "filename": file.filename,
                    "success": False,
                    "error": str(e),
                    "result": None
                })
        
        total_time = time.time() - start_time
        successful_count = sum(1 for r in results if r["success"])
        
        # æ·»åŠ åå°æ¸…ç†ä»»åŠ¡
        background_tasks.add_task(cleanup_memory)
        
        return {
            "success": True,
            "message": f"æ‰¹é‡åˆ†æå®Œæˆ: {successful_count}/{len(files)} æˆåŠŸ",
            "results": results,
            "total_processing_time": total_time,
            "successful_count": successful_count,
            "total_count": len(files)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}"
        )


@app.post("/api/optimize")
async def optimize_performance():
    """æ‰‹åŠ¨è§¦å‘æ€§èƒ½ä¼˜åŒ–"""
    try:
        # æ¸…ç†ç¼“å­˜
        with cache_lock:
            request_cache.clear()
        
        # ä¼˜åŒ–æ¨¡å‹å†…å­˜
        if model_manager:
            model_manager.optimize_memory()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        return {
            "success": True,
            "message": "æ€§èƒ½ä¼˜åŒ–å®Œæˆ",
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    
    # å¼€å‘ç¯å¢ƒè¿è¡Œ - æ€§èƒ½ä¼˜åŒ–é…ç½®
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
        workers=1,  # å•è¿›ç¨‹ä»¥é¿å…æ¨¡å‹é‡å¤åŠ è½½
        loop="asyncio",
        access_log=False  # ç¦ç”¨è®¿é—®æ—¥å¿—ä»¥æé«˜æ€§èƒ½
    )